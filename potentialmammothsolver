import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

from anagram import do_anagram
from hidden import ngrams_of, filter_real_words as hidden_filter
from selector import generate_all_selectors, filter_real_words as selector_filter

from glove import get_model


model = get_model()


def cosine_sim(v1, v2):
    if v1 is None or v2 is None:
        return 0.0
    denom = np.linalg.norm(v1) * np.linalg.norm(v2)
    if denom == 0:
        return 0.0
    return float(np.dot(v1, v2) / denom)


def get_vec(word):
    word = word.lower().strip()
    if word in model.key_to_index:
        return model[word]
    return None


def avg_similarity(indicator, ref_words):
    ivec = get_vec(indicator)
    sims = []
    for w in ref_words:
        sims.append(cosine_sim(ivec, get_vec(w)))
    return float(np.mean(sims)) if sims else 0.0


def avg_vec(text):
    toks = text.lower().split()
    vecs = [get_vec(t) for t in toks if get_vec(t) is not None]
    if not vecs:
        return None
    return np.mean(vecs, axis=0)


def best_definition_match(definition, candidates):
    dvec = avg_vec(definition)
    if dvec is None:
        return None, {c: 0.0 for c in candidates}

    scores = {}
    for c in candidates:
        wvec = get_vec(c)
        scores[c] = cosine_sim(dvec, wvec)

    best = max(scores, key=scores.get)
    return best, scores


ANAGRAM_WORDS = ["mix", "throwing", "destroy", "strange",
                 "dancing", "sort", "tampering", "exploded"]
HIDDEN_WORDS = ["hides", "displays", "reveals", "within", "held",
                "capturing", "absorbed", "sample", "selection", "bit", "taken"]
SELECTOR_WORDS = ["head", "tail", "heart", "borders", "coat",
                  "contents", "guts", "odd", "even", "alternate", "regularly"]


df = pd.read_csv("logistic_data.csv")

df["fodder_length"] = (
    df["fodder"]
    .astype(str)
    .str.replace(r"[^A-Za-z]", "", regex=True)
    .str.len()
)

df["fodder_word_count"] = df["fodder"].astype(str).str.split().apply(len)

df["glove_anagram"] = df["indicator"].apply(
    lambda x: avg_similarity(str(x), ANAGRAM_WORDS))
df["glove_hidden"] = df["indicator"].apply(
    lambda x: avg_similarity(str(x), HIDDEN_WORDS))
df["glove_selector"] = df["indicator"].apply(
    lambda x: avg_similarity(str(x), SELECTOR_WORDS))

X = df[[
    "length",
    "fodder_length",
    "fodder_word_count",
    "glove_anagram",
    "glove_hidden",
    "glove_selector"
]].values.astype(float)

y_raw = df["category"].values

label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y_raw)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.4, stratify=y, random_state=42
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

logreg = LogisticRegression(
    multi_class="multinomial",
    solver="lbfgs",
    max_iter=500
)
logreg.fit(X_train_scaled, y_train)

print("\n=== Classification Report ===")
print(classification_report(
    y_test, logreg.predict(X_test_scaled),
    target_names=label_encoder.classes_
))

print("\n=== Confusion Matrix ===")
print(confusion_matrix(y_test, logreg.predict(X_test_scaled)))


def extract_features(indicator, length, fodder):

    fod_clean_len = len("".join([c for c in str(fodder) if c.isalpha()]))
    fod_word_count = len(str(fodder).split())

    return np.array([[
        float(length),
        float(fod_clean_len),
        fod_word_count,
        avg_similarity(indicator, ANAGRAM_WORDS),
        avg_similarity(indicator, HIDDEN_WORDS),
        avg_similarity(indicator, SELECTOR_WORDS)
    ]])


def predict_category(indicator, length, fodder):
    x = extract_features(indicator, length, fodder)
    x_scaled = scaler.transform(x)

    pred_id = logreg.predict(x_scaled)[0]
    pred_label = label_encoder.inverse_transform([pred_id])[0]
    pred_probs = logreg.predict_proba(x_scaled)[0]

    return pred_label, pred_probs


def solve_anagram(fodder, length):
    words = do_anagram(fodder)
    return {w for w in words if len(w) == length}


def solve_hidden(fodder, length):
    grams = ngrams_of(length, fodder)
    return hidden_filter(grams)


def solve_selector(fodder, length):
    cands = generate_all_selectors(fodder, length)
    return selector_filter(cands)


print("\n=== Solve a New Cryptic Clue ===")
clue = input("Full clue: ")
indicator = input("Indicator word: ")
fodder = input("Fodder words: ")
definition = input("Definition: ")

while True:
    try:
        length = int(input("Solution length: "))
        break
    except:
        print("Length must be an integer.")

pred_label, pred_probs = predict_category(indicator, length, fodder)

print("\n=== Category Prediction ===")
print("Predicted Category:", pred_label)

print("\nProbabilities:")
for cat, p in zip(label_encoder.classes_, pred_probs):
    print(f"  {cat}: {p:.4f}")

print("\n=== Candidate Answers ===")

if pred_label == "Anagrams":
    candidates = solve_anagram(fodder, length)
elif pred_label == "Hiddens":
    candidates = solve_hidden(fodder, length)
elif pred_label == "Selectors":
    candidates = solve_selector(fodder, length)
else:
    candidates = set()

if not candidates:
    print("No English words found.")
    exit()

print("Candidates:", candidates)


if len(candidates) > 1:
    best, scores = best_definition_match(definition, candidates)

    print("\n=== Definition Meaning Match ===")
    print(f"Best Match: {best}\n")

    for w, s in sorted(scores.items(), key=lambda x: -x[1]):
        print(f"{w:<15} {s:.4f}")

    print("\nFinal Answer:", best)

else:
    print("\nFinal Answer:", list(candidates)[0])
